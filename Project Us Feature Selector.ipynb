{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as skl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from itertools import chain, combinations\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217\n",
      "513\n",
      "511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "X = pd.read_csv(\"data_preprocessed_python/eda_data.csv\")\n",
    "Y = pd.read_csv(\"data_preprocessed_python/eda_labels.csv\")\n",
    "X.rename(columns={\"Unnamed: 0\":\"video_num\"}, inplace=True)\n",
    "Y.rename(columns={\"Unnamed: 0\":\"video_num\"}, inplace=True)\n",
    "X.columns\n",
    "Y.columns\n",
    "\n",
    "Y_label = pd.DataFrame((Y['Arousal'])) #.astype(int)\n",
    "# Y_label['AVG_Arousal'] = np.where((Y_label['AVG_Arousal'] < 6.0), np.rint(Y_label['AVG_Arousal']), np.ceil(Y_label['AVG_Arousal']))\n",
    "# Y_label_avg = pd.DataFrame(Y['AVG_Arousal'])\n",
    "Y_label\n",
    "X_feature = X.drop(columns=['video_num','eda_filtered','onsets','peaks','variance','mean_eda'])\n",
    "X_features = X_feature[X_feature.notnull().all(axis=1)] #dropna(inplace=True)\n",
    "# X_features = np.array(X_features['std_dev']).reshape(-1,1)\n",
    "Y_labels = Y_label[X_feature.notnull().all(axis=1)]\n",
    "# Y_labels['AVG_Arousal'] = Y_labels['AVG_Arousal'].astype(int)\n",
    "# Y_labels_ex = Y_labels[(Y_labels['Arousal'] <= 3.0) | (Y_labels['Arousal'] >= 6.0)]\n",
    "Y_labels['classes'] = np.where((Y_labels['Arousal'] <= 3.0), -1, (np.where((Y_labels['Arousal'] <= 6.0), 0, 1)))\n",
    "\n",
    "# X_features = X_features[Y_labels.classes != 0]\n",
    "# Y_labels = Y_labels[Y_labels.classes != 0]\n",
    "\n",
    "print(len(Y_labels[Y_labels.classes == -1]))\n",
    "print(len(Y_labels[Y_labels.classes == 0]))\n",
    "print(len(Y_labels[Y_labels.classes == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09015413 0.08584321 0.09134426 0.08759504 0.09507718 0.0911929\n",
      " 0.08965959 0.0956274  0.08729179 0.09721599 0.0889985 ]\n",
      "Index(['std_dev', 'max_diff', 'skewness', 'median_eda', 'num_peaks',\n",
      "       'mean_amp', 'max_amp', 'max_eda', 'min_eda', 'rise_time', 'kurt'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('rise_time', 0.09721599042941491),\n",
       " ('max_eda', 0.09562739802458732),\n",
       " ('num_peaks', 0.09507718171119568),\n",
       " ('skewness', 0.09134425878031086),\n",
       " ('mean_amp', 0.09119290476584849),\n",
       " ('std_dev', 0.09015413393235006),\n",
       " ('max_amp', 0.08965958870997016),\n",
       " ('kurt', 0.08899849838441856),\n",
       " ('median_eda', 0.0875950443019232),\n",
       " ('min_eda', 0.08729178934667572),\n",
       " ('max_diff', 0.0858432116133051)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ExtraTreesClassifier(n_estimators=100)\n",
    "model.fit(X_features, Y_labels['classes'])\n",
    "print(model.feature_importances_)\n",
    "print(X_features.columns)\n",
    "feature_weights = dict(zip(X_features.columns, model.feature_importances_))\n",
    "sorted(feature_weights.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def powerset(iterable):\n",
    "    \"\"\"\n",
    "    powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\n",
    "    \"\"\"\n",
    "    xs = list(iterable)\n",
    "    # note we return an iterator rather than a list\n",
    "    return chain.from_iterable(combinations(xs,n) for n in range(len(xs)+1))\n",
    "\n",
    "feature_subsets = list(map(list, powerset(X_features.columns))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg(X_train, X_test, y_train, y_test):\n",
    "    scalar = StandardScaler()\n",
    "    scalar.fit(X_train,y_train)\n",
    "    scaled_X_train = scalar.transform(X_train)\n",
    "    scaled_X_test = scalar.transform(X_test)\n",
    "    print(X_train.columns)\n",
    "    fs_log_reg = LogisticRegression(C=10).fit(scaled_X_train,y_train)\n",
    "\n",
    "    #training data accuracy \n",
    "    #removing rise time causes much less units to be classified is low arousal\n",
    "#     print(\"Training Data: \\n\")\n",
    "    fs_y_pred = fs_log_reg.predict(scaled_X_train)\n",
    "    R_train = recall_score(y_train,fs_y_pred,average=None)\n",
    "    P_train = precision_score(y_train,fs_y_pred,average=None)\n",
    "    F1_train = f1_score(y_train,fs_y_pred,average=None)\n",
    "#     print(\"Recall: \"+str(R_train),\"Precision: \"+str(P_train), \"F1 score: \"+str(F1_train))\n",
    "    score_train = fs_log_reg.score(scaled_X_train,y_train)\n",
    "#     print(fs_log_reg.coef_, fs_log_reg.score(scaled_X_train,y_train))\n",
    "    # print(X_features.columns)\n",
    "\n",
    "    matrix_train = confusion_matrix(y_train, fs_y_pred)\n",
    "    print(matrix_train)\n",
    "#     print(matrix)\n",
    "\n",
    "    #testing data accuracy \n",
    "#     print(\"\\n Testing Data: \\n\")\n",
    "    fs_y_pred = fs_log_reg.predict(scaled_X_test)\n",
    "    R_test = recall_score(y_test,fs_y_pred,average=None)\n",
    "    P_test = precision_score(y_test,fs_y_pred,average=None)\n",
    "    F1_test = f1_score(y_test,fs_y_pred,average=None)\n",
    "#     print(\"Recall: \"+str(R_test),\"Precision: \"+str(P_test), \"F1 score: \"+str(F1_test))\n",
    "    score_test = fs_log_reg.score(scaled_X_test,y_test)\n",
    "#     print(fs_log_reg.coef_, fs_log_reg.score(scaled_X_test,y_test))\n",
    "    # print(X_features.columns)\n",
    "\n",
    "    matrix_test = confusion_matrix(y_test, fs_y_pred)\n",
    "    print(matrix_test)\n",
    "    \n",
    "    return score_train, F1_train, score_test, F1_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc_features_test = []\n",
    "best_acc_test = 0.0\n",
    "best_acc_F1_test = []\n",
    "\n",
    "best_acc_features_train = []\n",
    "best_acc_train = 0.0\n",
    "best_acc_F1_train = []\n",
    "\n",
    "for sub in feature_subsets:\n",
    "#     print(sub)\n",
    "    if not sub or len(sub) < 2:\n",
    "        continue\n",
    "    X_features_sub = X_features[sub]\n",
    "#     print(X_features_sub)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_features_sub, Y_labels['classes'], test_size=0.20, random_state=42)\n",
    "    score_train, F1_train, score_test, F1_test = logreg(X_train, X_test, y_train, y_test)\n",
    "    if score_train > best_acc_train:\n",
    "        best_acc_train = score_train\n",
    "        best_acc_features_train = sub\n",
    "        best_acc_F1_train = F1_train\n",
    "    \n",
    "    if score_test > best_acc_test:\n",
    "        best_acc_test = score_test\n",
    "        best_acc_features_test = sub\n",
    "        best_acc_F1_test = F1_test\n",
    "\n",
    "X_features_sub = X_features[best_acc_features_test]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features_sub, Y_labels['classes'], test_size=0.20, random_state=42)\n",
    "print(logreg(X_train, X_test, y_train, y_test))\n",
    "    \n",
    "X_features_sub = X_features[best_acc_features_train]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features_sub, Y_labels['classes'], test_size=0.20, random_state=42)\n",
    "print(logreg(X_train, X_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
